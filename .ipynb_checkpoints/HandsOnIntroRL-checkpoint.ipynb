{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to import the libraries we need to run a Cart Pole game in an OpenAI Gym environment (software library developed to simulate and test RL algorithms). We will also import numpy, a helpful mathematical computing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll create the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run through episodes, let’s build a function that accepts the environment and a policy array as inputs. The function will play the game and return the score from an episode as output. We’ll also receive an observation of the game state after every action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, policy):\n",
    "    \n",
    "    observation = env.reset()\n",
    "    \n",
    "    # create variables to track game status, score, and hold observations at each time step\n",
    "    \n",
    "    score = 0\n",
    "    observations = [ ]\n",
    "    completed = False\n",
    "    \n",
    "    # play the game until it is done\n",
    "    \n",
    "    for i in range(3000):\n",
    "        \n",
    "        # record observations\n",
    "        \n",
    "        observations += [observation.tolist()]\n",
    "        \n",
    "        if completed:\n",
    "            break\n",
    "        \n",
    "        # use the policy to decide on an action\n",
    "        \n",
    "        result = np.dot(policy, observation)\n",
    "        \n",
    "        if result > 0:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = 0\n",
    "        \n",
    "        # take a step using the action (the env.step method returns a snapshot of the environment after the action is taken, the reward from that action, whether the episode is completed, and diagnostic data for debugging)\n",
    "        \n",
    "        observation, reward, completed, data = env.step(action)\n",
    "        \n",
    "        # record cumulative score\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "    # end the function by returning the cumulative score and full list of observations at each time step\n",
    "    \n",
    "    return score, observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now that our brave AI is able to play the game, let’s give it a policy to do so. In the absence of a clever strategy for devising a policy, we’ll start with random values centred around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 9.0\n"
     ]
    }
   ],
   "source": [
    "policy = np.random.rand(1,4) - 0.5\n",
    "score, observations = play(env, policy)\n",
    "print('Score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the script, how did our agent perform? Cart Pole has a maximum score of 500. In all likelihood, our agent yielded a very low score. A better strategy might be to generate lots of random policies and keep the one with the highest score. The approach is to use a variable that progressively retains the policy, observations, and score of the best-performing game so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 500.0\n"
     ]
    }
   ],
   "source": [
    "# create a tuple to hold the best score, observations, and policy\n",
    "\n",
    "best = (0, [], [])\n",
    "\n",
    "# generate 1000 random policies centred around 0 and keep the best performing one\n",
    "\n",
    "for _ in range(1000):\n",
    "    \n",
    "    policy = np.random.rand(1,4) - 0.5\n",
    "    \n",
    "    score, observations = play(env, policy)\n",
    "    \n",
    "    if score > best[0]:\n",
    "        best = (score, observations, policy)\n",
    "\n",
    "print('Best score:', best[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is our best score now? Chances are, we have come up with a policy that is able to achieve the high score of 500. Our agent has beat the game!\n",
    "\n",
    "Where do we go from here? Well that’s it for this post but if we wanted to build a more robust system we might consider some of the following approaches:\n",
    "\n",
    "- Using an optimization algorithm to find the best policy instead of randomly picking (e.g. Deep Q Learning, Proximal Policy Optimization, Monte Carlo Tree Search, etc.)\n",
    "- Testing the best policy that we obtained over many episodes to ensure that we didn’t just get lucky in the one episode\n",
    "- Testing our policy on a version of cart pole with a higher top score than 500 to see how sustainable the policy is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
